{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import re\n",
    "import emoji\n",
    "import pickle\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as downloader_api\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras import Sequential, Input #type:ignore\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, TextVectorization, Embedding # type:ignore\n",
    "\n",
    "# word2vec_model = downloader_api.load('word2vec-google-news-300')\n",
    "# glove_model = downloader_api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "word2vec_model = KeyedVectors.load(r'models\\artifacts\\word2vec.kv')\n",
    "glove_model = KeyedVectors.load(r'models\\artifacts\\glove.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word2vec_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124martifacts\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mword2vec.kv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m glove_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124martifacts\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mglove.kv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word2vec_model' is not defined"
     ]
    }
   ],
   "source": [
    "# word2vec_model.save(r'models\\artifacts\\word2vec.kv')\n",
    "# glove_model.save(r'models\\artifacts\\glove.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data paths for different types of data provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emotion Intensity Data\\\\Anger\\\\anger_dev.txt',\n",
       " 'Emotion Intensity Data\\\\Anger\\\\anger_test.txt',\n",
       " 'Emotion Intensity Data\\\\Anger\\\\anger_train.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anger_path = glob('Emotion Intensity Data\\Anger\\*')\n",
    "anger_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emotion Intensity Data\\\\Fear\\\\fear_dev.txt',\n",
       " 'Emotion Intensity Data\\\\Fear\\\\fear_test.txt',\n",
       " 'Emotion Intensity Data\\\\Fear\\\\fear_train.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fear_path = glob('Emotion Intensity Data\\Fear\\*')\n",
    "fear_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emotion Intensity Data\\\\Joy\\\\joy_dev.txt',\n",
       " 'Emotion Intensity Data\\\\Joy\\\\joy_test.txt',\n",
       " 'Emotion Intensity Data\\\\Joy\\\\joy_train.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joy_path = glob('Emotion Intensity Data\\Joy\\*')\n",
    "joy_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emotion Intensity Data\\\\Sadness\\\\sadness_dev.txt',\n",
       " 'Emotion Intensity Data\\\\Sadness\\\\sadness_test.txt',\n",
       " 'Emotion Intensity Data\\\\Sadness\\\\sadness_train.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sadness_path = glob('Emotion Intensity Data\\Sadness\\*')\n",
    "sadness_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(file_paths: list):\n",
    "    train_dataframe_0 = pd.read_csv(file_paths[0], sep='\\t', header=None, names=['ID','Comment','Emotion','Intensity'])\n",
    "    train_dataframe_1 = pd.read_csv(file_paths[2], sep='\\t', header=None, names=['ID','Comment','Emotion','Intensity'])\n",
    "    train_df = pd.concat([train_dataframe_1,train_dataframe_0]).reset_index(drop=True)\n",
    "    test_df = pd.read_csv(file_paths[1], sep='\\t', header=None, names=['ID','Comment','Emotion','Intensity'])\n",
    "    return (train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry_train_df, angry_test_df = data_loader(anger_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_train_df, fear_test_df = data_loader(fear_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "joy_train_df, joy_test_df = data_loader(joy_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sadness Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadness_train_df, sadness_test_df = data_loader(sadness_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "- Convertion to lowercase\n",
    "- Removal of:\n",
    "    - Whitespace charecters\n",
    "    - All other Special Charecters\n",
    "    - Stopwords\n",
    "    - Emojis\n",
    "- Tokenization, Stemming, Lemmatization\n",
    "- Feature Extraction of stemmed and lemmatized text using:\n",
    "    - Bag of Words\n",
    "    - TF/IDF\n",
    "    - Word2Vec\n",
    "    - GloVe\n",
    "\n",
    "We will create a class that does this job overall and returns the stemmed and lemmatized data that has undergone all the feature extraction techniques mentioned above. Therefore there will be a total of 8 types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_preprocess():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self, df: pd.DataFrame, return_df: bool = False):\n",
    "        stem_bow_vectorizer = CountVectorizer()\n",
    "        lemmatize_bow_vectorizer = CountVectorizer()\n",
    "        stemmed_tf_idf_vectorizer = TfidfVectorizer()\n",
    "        lemmatized_tf_idf_vectorizer = TfidfVectorizer()\n",
    "        \n",
    "        df['Cleaned_text'] = df['Comment'].str.lower()                              # convert to lowercase\n",
    "        df['Cleaned_text'] = df['Cleaned_text'].apply(self.remove_whitespace)       # removes whitespace charecters\n",
    "        df['Cleaned_text'] = df['Cleaned_text'].apply(self.remove_special_chars)    # removes all other special charecters\n",
    "        df['Cleaned_text'] = df['Cleaned_text'].apply(self.remove_stopwords)        # removes stopwords        \n",
    "        df['Cleaned_text'] = df['Cleaned_text'].apply(self.remove_emoji)            # removes emojis\n",
    "\n",
    "        df['Stemmed_text'] = df['Cleaned_text'].apply(self.tokenize_stem)           # applies tokenization & stemming on cleaned text\n",
    "        df['Lemmatized_text'] = df['Cleaned_text'].apply(self.tokenize_lemmatize)   # applies tokenization & lemmatization on cleaned text\n",
    "        \n",
    "        stemmed_bow = pd.DataFrame(stem_bow_vectorizer.fit_transform(df['Stemmed_text']).toarray())             # Stemmed data - BOW\n",
    "        lemmatized_bow = pd.DataFrame(lemmatize_bow_vectorizer.fit_transform(df['Lemmatized_text']).toarray())  # Lemmatized data - BOW\n",
    "        \n",
    "        stemmed_tf_idf = pd.DataFrame(stemmed_tf_idf_vectorizer.fit_transform(df['Stemmed_text']).toarray())    # Stemmed data - TF/IDF\n",
    "        lemmatized_tf_idf = pd.DataFrame(lemmatized_tf_idf_vectorizer.fit_transform(df['Lemmatized_text']).toarray()) # Lemmatized data - TF/IDF\n",
    "\n",
    "        # load the word2vec model & vectorize the sentence\n",
    "        stemmed_word2vec = pd.DataFrame(np.vstack(df['Stemmed_text'].apply(lambda x: self.vec_converter(sentence = x, \n",
    "                                                                                                           keyedvector = word2vec_model))))\n",
    "        lemmatized_word2vec = pd.DataFrame(np.vstack(df['Lemmatized_text'].apply(lambda x: self.vec_converter(sentence = x, \n",
    "                                                                                                                 keyedvector = word2vec_model))))\n",
    "        \n",
    "        # load the glove model & vectorize the sentence\n",
    "        stemmed_glove = pd.DataFrame(np.vstack(df['Stemmed_text'].apply(lambda x: self.vec_converter(sentence = x, \n",
    "                                                                                                           keyedvector = glove_model))))\n",
    "        lemmatized_glove = pd.DataFrame(np.vstack(df['Lemmatized_text'].apply(lambda x: self.vec_converter(sentence = x, \n",
    "                                                                                                                 keyedvector = glove_model))))\n",
    "        \n",
    "        if return_df is True:\n",
    "            return (df)\n",
    "        else:\n",
    "            self.save(stem_bow_vectorizer,r'models\\artifacts\\stem_bow_vectorizer.pkl')\n",
    "            self.save(lemmatize_bow_vectorizer,r'models\\artifacts\\lemmatize_bow_vectorizer.pkl')\n",
    "            self.save(stemmed_tf_idf_vectorizer,r'models\\artifacts\\stem_tf_idf_vectorizer.pkl')\n",
    "            self.save(lemmatized_tf_idf_vectorizer,r'models\\artifacts\\lemmatize_tf_idf_vectorizer.pkl')\n",
    "            return((stemmed_bow, lemmatized_bow, stemmed_tf_idf, lemmatized_tf_idf, \n",
    "                   stemmed_word2vec, lemmatized_word2vec, stemmed_glove, lemmatized_glove))\n",
    "    \n",
    "    def remove_whitespace(self, text:str):\n",
    "        pattern = r'\\\\[tnr\\x0b\\x0c]'\n",
    "        text = re.sub(pattern, ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_special_chars(self, text:str):\n",
    "        text = re.sub(pattern=\"[^a-zA-Z0-9]\",\n",
    "                    repl=\" \",\n",
    "                    string=text)\n",
    "        text = re.sub(pattern=\"\\s+\",\n",
    "                    repl=\" \",\n",
    "                    string=text)\n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text:str):\n",
    "        stopwords_ = stopwords.words('english')\n",
    "        stopwords_.extend([\"i'm\",\"im\",\"u\"])\n",
    "        return(\" \".join([word for word in text.split() if word not in stopwords_]))\n",
    "    \n",
    "    def remove_emoji(self, text:str):\n",
    "        return(emoji.replace_emoji(text,\"\"))\n",
    "    \n",
    "    def tokenize_stem(self, sentence:str):\n",
    "        stemmer = PorterStemmer()\n",
    "        tokenized_sentence = word_tokenize(sentence)\n",
    "        stemmed_tokens = [stemmer.stem(word) for word in tokenized_sentence]\n",
    "        stemmed_sentence = \" \".join(stemmed_tokens)\n",
    "        return stemmed_sentence\n",
    "    \n",
    "    def tokenize_lemmatize(self, sentence:str):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokenized_sentence = word_tokenize(sentence)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokenized_sentence]\n",
    "        lemmatized_sentence = \" \".join(lemmatized_tokens)\n",
    "        return lemmatized_sentence\n",
    "    \n",
    "    def vec_converter(self, sentence:str, keyedvector):\n",
    "        tokenized_sentence = word_tokenize(sentence)\n",
    "        vector_token = [keyedvector[word] for word in tokenized_sentence if word in keyedvector] #this is where we convert the tokens into vectors\n",
    "        vector = np.mean(vector_token,axis=0) if vector_token else np.zeros(keyedvector.vector_size)\n",
    "        return vector\n",
    "    \n",
    "    def save(self, object, filepath):\n",
    "        with open(filepath, 'wb') as file:\n",
    "            pickle.dump(object, file)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        with open(filepath, 'rb') as file:\n",
    "            object = pickle.load(file)\n",
    "        return (object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_obj = text_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluate():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def eval(self, y_true, y_pred):\n",
    "        pearsonr_ = pearsonr(y_true, y_pred)\n",
    "        spearmanr_ = spearmanr(y_true, y_pred)\n",
    "        return ({\"Pearson\":pearsonr_.statistic,\"Spearman\":spearmanr_.statistic})\n",
    "    \n",
    "eval_obj = evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "We will use two types of models:\n",
    "- Statistical models\n",
    "- Deep Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Models\n",
    "We will use the following statistical models from the statsmodels library\n",
    "- OLS (Ordinary Least Squares)\n",
    "- GLS (Generalized Least Squares)\n",
    "- WLS (Weighted Least Squares)\n",
    "- GLM (Generalized Linear Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class statmodels():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    # common function for all the selected statsmodels\n",
    "    def stats_models(self, X, y, model_name:str, data_name:str, df_data_name:str):\n",
    "        X = sm.add_constant(X)\n",
    "        x_train,x_test,y_train,y_test = train_test_split(X, y, train_size=0.75, random_state=42)\n",
    "\n",
    "        # if model_name in 'OLS':\n",
    "        #     model = sm.OLS(endog=y_train,\n",
    "        #                 exog=x_train)\n",
    "        # elif model_name in \"GLS\":\n",
    "        #     model = sm.GLS(endog=y_train,\n",
    "        #                 exog=x_train)\n",
    "        # elif model_name == \"WLS\":\n",
    "        #     model = sm.WLS(endog=y_train,\n",
    "        #                 exog=x_train)\n",
    "        # elif model_name == \"GLM\":\n",
    "        #     model = sm.GLM(endog=y_train,\n",
    "        #                    exog=x_train)\n",
    "\n",
    "        model = sm.OLS(endog=y_train, exog=x_train)\n",
    "        result = model.fit()\n",
    "        y_pred = result.predict(x_test)\n",
    "        result.save(f'models\\statsmodels\\{model_name}_{data_name}_{df_data_name}.pkl') # saving the instance of model for later usage\n",
    "        result = eval_obj.eval(y_test,y_pred)\n",
    "        return result\n",
    "    \n",
    "    def stats_models_predict(self, X: pd.DataFrame, model,):\n",
    "        X = sm.add_constant(X)\n",
    "        y_pred = model.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def model_report(self,df: pd.DataFrame, df_data_name_: str):\n",
    "        processed_data = (stemmed_bow, lemmatized_bow, stemmed_tf_idf, lemmatized_tf_idf, \n",
    "                          stemmed_word2vec, lemmatized_word2vec, stemmed_glove, lemmatized_glove) = preprocessor_obj.preprocess(df)\n",
    "        data_names = ['Stem_BOW','Lemmatize_BOW','Stem_TF_IDF','Lemmatize_TF_IDF',\n",
    "                    'Stem_Word2Vec','Lemmatize_Word2Vec','Stem_GloVe','Lemmatize_GloVe']\n",
    "        stats_models_names = ['OLS']#,'GLS','WLS','GLM']\n",
    "        stats_report = {}\n",
    "        for model_name in stats_models_names:\n",
    "            stats_report[model_name] = {}\n",
    "            for i in range(len(processed_data)):  \n",
    "                stats_report[model_name][data_names[i]] = self.stats_models(processed_data[i], df['Intensity'], \n",
    "                                                                            model_name=model_name,data_name=data_names[i],\n",
    "                                                                            df_data_name = df_data_name_)\n",
    "        return stats_report['OLS']\n",
    "    \n",
    "stat_model = statmodels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statsmodel_test_prediction(df: pd.DataFrame, model_name: str, stem_or_lemma: str,\n",
    "                               feature_extraction_name: str, data_name: str):\n",
    "    with open(f'models\\statsmodels\\{model_name}_{stem_or_lemma}_{feature_extraction_name}_{data_name}.pkl','rb') as file:\n",
    "        model_result = pickle.load(file)\n",
    "    processed_df = preprocessor_obj.preprocess(df, return_df=True)\n",
    "    if stem_or_lemma in 'Stem':\n",
    "        if feature_extraction_name in 'BOW':\n",
    "            vectorizer = preprocessor_obj.load(r'models\\artifacts\\stem_bow_vectorizer.pkl')\n",
    "            stemmed_df = pd.DataFrame(vectorizer.transform(processed_df['Stemmed_text']).toarray())\n",
    "        elif feature_extraction_name in 'TF_IDF':\n",
    "            vectorizer = preprocessor_obj.load(r'models\\artifacts\\stem_tf_idf_vectorizer.pkl')\n",
    "            stemmed_df = pd.DataFrame(vectorizer.transform(processed_df['Stemmed_text']).toarray())\n",
    "        elif feature_extraction_name in 'Word2Vec':\n",
    "            stemmed_df = pd.DataFrame(np.vstack(processed_df['Stemmed_text'].apply(lambda x: preprocessor_obj.vec_converter(sentence = x, \n",
    "                                                                                                   keyedvector = word2vec_model))))\n",
    "        elif feature_extraction_name in 'GloVe':\n",
    "            stemmed_df = pd.DataFrame(np.vstack(processed_df['Stemmed_text'].apply(lambda x: preprocessor_obj.vec_converter(sentence = x, \n",
    "                                                                                                   keyedvector = glove_model))))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown feature extraction name: {feature_extraction_name}\")\n",
    "        y_pred = stat_model.stats_models_predict(stemmed_df,model_result)\n",
    "\n",
    "    elif stem_or_lemma in 'Lemmatize':\n",
    "        if feature_extraction_name in 'BOW':\n",
    "            vectorizer = preprocessor_obj.load(r'models\\artifacts\\lemmatize_bow_vectorizer.pkl')\n",
    "            lemmatized_df = pd.DataFrame(vectorizer.transform(processed_df['Lemmatized_text']).toarray())\n",
    "        elif feature_extraction_name in 'TF_IDF':\n",
    "            vectorizer = preprocessor_obj.load(r'models\\artifacts\\lemmatize_tf_idf_vectorizer.pkl')\n",
    "            lemmatized_df = pd.DataFrame(vectorizer.transform(processed_df['Lemmatized_text']).toarray())\n",
    "        elif feature_extraction_name in 'Word2Vec':\n",
    "            lemmatized_df = pd.DataFrame(np.vstack(processed_df['Lemmatized_text'].apply(lambda x: preprocessor_obj.vec_converter(sentence = x, \n",
    "                                                                                                   keyedvector = word2vec_model))))\n",
    "        elif feature_extraction_name in 'GloVe':\n",
    "            lemmatized_df = pd.DataFrame(np.vstack(processed_df['Lemmatized_text'].apply(lambda x: preprocessor_obj.vec_converter(sentence = x, \n",
    "                                                                                                   keyedvector = glove_model))))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown feature extraction name: {feature_extraction_name}\")\n",
    "        y_pred = stat_model.stats_models_predict(lemmatized_df,model_result)\n",
    "\n",
    "    return (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_stat_report = pd.DataFrame(stat_model.model_report(fear_train_df.copy(),'Fear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Stem_BOW</th>\n",
       "      <td>0.479338</td>\n",
       "      <td>0.543866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lemmatize_BOW</th>\n",
       "      <td>0.468752</td>\n",
       "      <td>0.521532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stem_TF_IDF</th>\n",
       "      <td>0.471711</td>\n",
       "      <td>0.510255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lemmatize_TF_IDF</th>\n",
       "      <td>0.535501</td>\n",
       "      <td>0.590652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stem_Word2Vec</th>\n",
       "      <td>0.314629</td>\n",
       "      <td>0.329086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lemmatize_Word2Vec</th>\n",
       "      <td>0.534867</td>\n",
       "      <td>0.538858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stem_GloVe</th>\n",
       "      <td>0.340748</td>\n",
       "      <td>0.355140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lemmatize_GloVe</th>\n",
       "      <td>0.469166</td>\n",
       "      <td>0.481395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Pearson  Spearman\n",
       "Stem_BOW            0.479338  0.543866\n",
       "Lemmatize_BOW       0.468752  0.521532\n",
       "Stem_TF_IDF         0.471711  0.510255\n",
       "Lemmatize_TF_IDF    0.535501  0.590652\n",
       "Stem_Word2Vec       0.314629  0.329086\n",
       "Lemmatize_Word2Vec  0.534867  0.538858\n",
       "Stem_GloVe          0.340748  0.355140\n",
       "Lemmatize_GloVe     0.469166  0.481395"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fear_stat_report.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = statsmodel_test_prediction(fear_test_df,\n",
    "                                    'OLS',\n",
    "                                    'Lemmatize',\n",
    "                                    'GloVe',\n",
    "                                    'Fear')\n",
    "result = eval_obj.eval(fear_test_df['Intensity'],y_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = input(\"Test on Stem/Lemmatized test data: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stem'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pearson': 0.5224373248449986, 'Spearman': 0.5064857798464922}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Models\n",
    "We will use the following models:\n",
    "- RNN (Recurrent Neural Networks)\n",
    "- LSTM (Long Short Term Memory)\n",
    "- GRU (Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_models():\n",
    "    def __init__(self) -> None:\n",
    "        # The maximum length of a sentence that can be accepted by the model:\n",
    "        #<<<< max(len(data) for data in train_df['Lemmatized_text']) = 127 >>>>#\n",
    "        # Running the above code on Lemmatized data tells us that the maximum length of a sentence in lemmatized text is 127. \n",
    "        # Hence we will round it to 130. \n",
    "        self.sequence_length = 130\n",
    "        self.nn_models = [SimpleRNN, LSTM, GRU]\n",
    "        self.nn_model_names = ['RNN','LSTM','GRU']\n",
    "        self.data_type_names = ['Stemmed','Lemmatized']\n",
    "\n",
    "    def data_splitter(self,X,y):\n",
    "        x_train,x_test,y_train,y_test = train_test_split(X, y, train_size=0.75, random_state=42)\n",
    "        return x_train,x_test,y_train,y_test\n",
    "    \n",
    "    def stem_data(self, df: pd.DataFrame):\n",
    "        #split the data into train and test to avoid data leakage\n",
    "        x_train_stemmed,x_test_stemmed,y_train_stemmed,y_test_stemmed = self.data_splitter(df['Stemmed_text'],df['Intensity'])\n",
    "\n",
    "        #perform tokenization using TextVectorization\n",
    "        text_vectorizer_stemmed = TextVectorization(output_sequence_length=self.sequence_length)\n",
    "        text_vectorizer_stemmed.adapt(df['Stemmed_text'])\n",
    "        # vocab_size = text_vectorizer_stemmed.vocabulary_size()\n",
    "\n",
    "        # #Get the data in tensor format\n",
    "        # x_train_stemmed = text_vectorizer_stemmed(x_train)\n",
    "        # x_test_stemmed = text_vectorizer_stemmed(x_test)\n",
    "        # y_train_stemmed = tf.convert_to_tensor(y_train)\n",
    "        # y_test_stemmed = tf.convert_to_tensor(y_test)\n",
    "        \n",
    "        # save(text_vectorizer_stemmed, r\"models\\artifacts\\text_vectorizer_stemmed\")\n",
    "\n",
    "        return (x_train_stemmed, x_test_stemmed, y_train_stemmed, y_test_stemmed, text_vectorizer_stemmed)\n",
    "    \n",
    "    def lemmatize_data(self, df: pd.DataFrame):\n",
    "        #split the data into train and test to avoid data leakage\n",
    "        x_train_lemmatized,x_test_lemmatized,y_train_lemmatized,y_test_lemmatized = self.data_splitter(df['Lemmatized_text'],df['Intensity'])\n",
    "\n",
    "        #perform tokenization using TextVectorization\n",
    "        text_vectorizer_lemmatized = TextVectorization(output_sequence_length=self.sequence_length)\n",
    "        text_vectorizer_lemmatized.adapt(df['Lemmatized_text'])\n",
    "        # vocab_size = text_vectorizer_lemmatized.vocabulary_size()\n",
    "\n",
    "        # #Get the data in tensor format\n",
    "        # x_train_lemmatized = text_vectorizer_lemmatized(x_train)\n",
    "        # x_test_lemmatized = text_vectorizer_lemmatized(x_test)\n",
    "        # y_train_lemmatized = tf.convert_to_tensor(y_train)\n",
    "        # y_test_lemmatized = tf.convert_to_tensor(y_test)\n",
    "\n",
    "        # save(text_vectorizer_lemmatized, r\"models\\artifacts\\text_vectorizer_lemmatized\")\n",
    "\n",
    "        return (x_train_lemmatized, x_test_lemmatized, y_train_lemmatized, y_test_lemmatized, text_vectorizer_lemmatized)\n",
    "    \n",
    "\n",
    "    def model_builder(self, model_, text_vectorizer):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(1,), dtype=tf.string))\n",
    "        model.add(text_vectorizer)\n",
    "        model.add(Embedding(input_dim=text_vectorizer.vocabulary_size(),\n",
    "                        output_dim=64,\n",
    "                        input_length=self.sequence_length))\n",
    "        model.add(model_(64,return_sequences=True))\n",
    "        model.add(model_(32))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "        return (model)\n",
    "    \n",
    "    def load_nn_model(self,file_path:str):\n",
    "        return load_model(file_path)\n",
    "    \n",
    "    def nn_model_report(self, df: pd.DataFrame, df_data_name_: str):\n",
    "        df = preprocessor_obj.preprocess(df, return_df=True)\n",
    "        (x_train_stemmed, x_test_stemmed, y_train_stemmed, y_test_stemmed, text_vectorizer_stemmed) = self.stem_data(df)\n",
    "        (x_train_lemmatized, x_test_lemmatized, y_train_lemmatized, y_test_lemmatized, text_vectorizer_lemmatized) = self.lemmatize_data(df)\n",
    "        text_vectorizers = [text_vectorizer_stemmed, text_vectorizer_lemmatized]\n",
    "        train_data = [[x_train_stemmed,y_train_stemmed],[x_train_lemmatized,y_train_lemmatized]]\n",
    "        test_data = [[x_test_stemmed,y_test_stemmed],[x_test_lemmatized,y_test_lemmatized]]\n",
    "\n",
    "        stats_report = {}\n",
    "        for x in range(len(self.nn_models)):\n",
    "            clear_session()\n",
    "            model = self.model_builder(self.nn_models[x], text_vectorizers[0])\n",
    "            stats_report[self.nn_model_names[x]] = {}\n",
    "            for k,(i,j) in enumerate(zip(train_data,test_data)):\n",
    "                print(model.summary(),'\\n')\n",
    "                model.fit(x=tf.convert_to_tensor(i[0]), \n",
    "                        y=tf.convert_to_tensor(i[1]),\n",
    "                        epochs=20,\n",
    "                        validation_data=(tf.convert_to_tensor(j[0]),tf.convert_to_tensor(j[1])))\n",
    "                model.save(f'models\\deep_learning_models\\{self.nn_model_names[x]}_{self.data_type_names[k]}_{df_data_name_}.tf')\n",
    "                # rmse = pow(model.get_metrics_result()['loss'].numpy(),0.5)\n",
    "                y_pred = model.predict(tf.convert_to_tensor(j[0]))\n",
    "                result = eval_obj.eval(j[1],y_pred.flatten())\n",
    "                stats_report[self.nn_model_names[x]][self.data_type_names[k]] = result\n",
    "                clear_session()\n",
    "                model = self.model_builder(self.nn_models[x],text_vectorizers[1])  \n",
    "        return stats_report\n",
    "    \n",
    "    def nn_models_test_prediction(self, model_filepath:str, df: pd.DataFrame, stem_or_lemma:str):\n",
    "        df = preprocessor_obj.preprocess(df, return_df=True)\n",
    "        model = self.load_nn_model(model_filepath)\n",
    "        if stem_or_lemma == 'Stem':\n",
    "            y_pred = model.predict(tf.convert_to_tensor(df['Stemmed_text']))\n",
    "        elif stem_or_lemma == 'Lemmatize':\n",
    "            y_pred = model.predict(tf.convert_to_tensor(df['Lemmatized_text']))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown data name: {stem_or_lemma}\")\n",
    "        return y_pred\n",
    "\n",
    "nn_models_obj = nn_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVe  (None, 130)               0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 130, 64)           249472    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 130, 64)           8256      \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 32)                3104      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 260865 (1019.00 KB)\n",
      "Trainable params: 260865 (1019.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None \n",
      "\n",
      "Epoch 1/20\n",
      "30/30 [==============================] - 4s 53ms/step - loss: 0.0645 - val_loss: 0.0360\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0402 - val_loss: 0.0354\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.0345 - val_loss: 0.0359\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0218 - val_loss: 0.0328\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.0139 - val_loss: 0.0393\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0122 - val_loss: 0.0375\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.0085 - val_loss: 0.0347\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0065 - val_loss: 0.0395\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.0060 - val_loss: 0.0356\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.0047 - val_loss: 0.0377\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.0036 - val_loss: 0.0389\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.0037 - val_loss: 0.0411\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.0040 - val_loss: 0.0365\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 0.0038 - val_loss: 0.0373\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 0.0036 - val_loss: 0.0367\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 1s 46ms/step - loss: 0.0031 - val_loss: 0.0363\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 0.0031 - val_loss: 0.0370\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0027 - val_loss: 0.0399\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.0029 - val_loss: 0.0344\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0028 - val_loss: 0.0387\n",
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\RNN_Stemmed_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\RNN_Stemmed_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 8ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, 130)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 130, 64)           272832    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 130, 64)           8256      \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 32)                3104      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 284225 (1.08 MB)\n",
      "Trainable params: 284225 (1.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None \n",
      "\n",
      "Epoch 1/20\n",
      "30/30 [==============================] - 5s 73ms/step - loss: 0.0608 - val_loss: 0.0348\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.0398 - val_loss: 0.0357\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 0.0404 - val_loss: 0.0364\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.0393 - val_loss: 0.0342\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.0407 - val_loss: 0.0342\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.0377 - val_loss: 0.0359\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.0256 - val_loss: 0.0410\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.0208 - val_loss: 0.0408\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.0175 - val_loss: 0.0404\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.0169 - val_loss: 0.0355\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.0151 - val_loss: 0.0385\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.0102 - val_loss: 0.0411\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.0071 - val_loss: 0.0388\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.0069 - val_loss: 0.0444\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.0061 - val_loss: 0.0461\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.0047 - val_loss: 0.0444\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 0.0043 - val_loss: 0.0444\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.0041 - val_loss: 0.0430\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0044 - val_loss: 0.0489\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.0043 - val_loss: 0.0473\n",
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\RNN_Lemmatized_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\RNN_Lemmatized_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 9ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVe  (None, 130)               0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 130, 64)           249472    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 130, 64)           33024     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 294945 (1.13 MB)\n",
      "Trainable params: 294945 (1.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None \n",
      "\n",
      "Epoch 1/20\n",
      "30/30 [==============================] - 8s 133ms/step - loss: 0.0681 - val_loss: 0.0361\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 0.0396 - val_loss: 0.0349\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 3s 99ms/step - loss: 0.0391 - val_loss: 0.0342\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 3s 95ms/step - loss: 0.0394 - val_loss: 0.0355\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 3s 108ms/step - loss: 0.0396 - val_loss: 0.0361\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 3s 106ms/step - loss: 0.0395 - val_loss: 0.0342\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.0396 - val_loss: 0.0372\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 3s 96ms/step - loss: 0.0389 - val_loss: 0.0340\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 3s 115ms/step - loss: 0.0397 - val_loss: 0.0350\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 4s 125ms/step - loss: 0.0393 - val_loss: 0.0345\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 3s 110ms/step - loss: 0.0399 - val_loss: 0.0340\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 4s 121ms/step - loss: 0.0402 - val_loss: 0.0344\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 3s 102ms/step - loss: 0.0400 - val_loss: 0.0343\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 3s 114ms/step - loss: 0.0389 - val_loss: 0.0344\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 3s 99ms/step - loss: 0.0389 - val_loss: 0.0350\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 3s 103ms/step - loss: 0.0401 - val_loss: 0.0345\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 3s 90ms/step - loss: 0.0389 - val_loss: 0.0342\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 3s 95ms/step - loss: 0.0390 - val_loss: 0.0343\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.0393 - val_loss: 0.0340\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 4s 131ms/step - loss: 0.0398 - val_loss: 0.0341\n",
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\LSTM_Stemmed_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\LSTM_Stemmed_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 25ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, 130)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 130, 64)           272832    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 130, 64)           33024     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 318305 (1.21 MB)\n",
      "Trainable params: 318305 (1.21 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None \n",
      "\n",
      "Epoch 1/20\n",
      "30/30 [==============================] - 10s 127ms/step - loss: 0.0662 - val_loss: 0.0380\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 3s 110ms/step - loss: 0.0400 - val_loss: 0.0350\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 4s 129ms/step - loss: 0.0407 - val_loss: 0.0358\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 4s 129ms/step - loss: 0.0399 - val_loss: 0.0342\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 4s 146ms/step - loss: 0.0389 - val_loss: 0.0343\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 5s 153ms/step - loss: 0.0395 - val_loss: 0.0352\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 4s 141ms/step - loss: 0.0400 - val_loss: 0.0370\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 5s 157ms/step - loss: 0.0394 - val_loss: 0.0351\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 4s 130ms/step - loss: 0.0393 - val_loss: 0.0351\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 4s 138ms/step - loss: 0.0393 - val_loss: 0.0357\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 0.0406 - val_loss: 0.0342\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 4s 122ms/step - loss: 0.0389 - val_loss: 0.0341\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 4s 125ms/step - loss: 0.0395 - val_loss: 0.0340\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 4s 125ms/step - loss: 0.0395 - val_loss: 0.0342\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 4s 132ms/step - loss: 0.0401 - val_loss: 0.0347\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 4s 125ms/step - loss: 0.0399 - val_loss: 0.0341\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 3s 116ms/step - loss: 0.0392 - val_loss: 0.0340\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.0391 - val_loss: 0.0340\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 4s 127ms/step - loss: 0.0391 - val_loss: 0.0344\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 4s 125ms/step - loss: 0.0397 - val_loss: 0.0343\n",
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\LSTM_Lemmatized_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\LSTM_Lemmatized_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 25ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVe  (None, 130)               0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 130, 64)           249472    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 130, 64)           24960     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                9408      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 283873 (1.08 MB)\n",
      "Trainable params: 283873 (1.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None \n",
      "\n",
      "Epoch 1/20\n",
      "30/30 [==============================] - 9s 146ms/step - loss: 0.0864 - val_loss: 0.0362\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 4s 118ms/step - loss: 0.0404 - val_loss: 0.0342\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 4s 130ms/step - loss: 0.0390 - val_loss: 0.0340\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 4s 129ms/step - loss: 0.0391 - val_loss: 0.0340\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 3s 111ms/step - loss: 0.0391 - val_loss: 0.0342\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 4s 127ms/step - loss: 0.0392 - val_loss: 0.0340\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 4s 128ms/step - loss: 0.0390 - val_loss: 0.0341\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 3s 116ms/step - loss: 0.0392 - val_loss: 0.0341\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 0.0391 - val_loss: 0.0348\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 3s 117ms/step - loss: 0.0392 - val_loss: 0.0340\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 4s 123ms/step - loss: 0.0390 - val_loss: 0.0345\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 3s 111ms/step - loss: 0.0394 - val_loss: 0.0342\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 4s 122ms/step - loss: 0.0390 - val_loss: 0.0351\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 4s 118ms/step - loss: 0.0391 - val_loss: 0.0354\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.0390 - val_loss: 0.0340\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 4s 130ms/step - loss: 0.0396 - val_loss: 0.0340\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 4s 129ms/step - loss: 0.0390 - val_loss: 0.0340\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 4s 122ms/step - loss: 0.0391 - val_loss: 0.0352\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 4s 128ms/step - loss: 0.0394 - val_loss: 0.0347\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 4s 141ms/step - loss: 0.0394 - val_loss: 0.0341\n",
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\GRU_Stemmed_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\GRU_Stemmed_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 30ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, 130)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 130, 64)           272832    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 130, 64)           24960     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                9408      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 307233 (1.17 MB)\n",
      "Trainable params: 307233 (1.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None \n",
      "\n",
      "Epoch 1/20\n",
      "30/30 [==============================] - 9s 127ms/step - loss: 0.0708 - val_loss: 0.0342\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 4s 143ms/step - loss: 0.0391 - val_loss: 0.0341\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 4s 149ms/step - loss: 0.0390 - val_loss: 0.0340\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 5s 176ms/step - loss: 0.0392 - val_loss: 0.0349\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 5s 163ms/step - loss: 0.0395 - val_loss: 0.0383\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 4s 148ms/step - loss: 0.0395 - val_loss: 0.0341\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 4s 151ms/step - loss: 0.0394 - val_loss: 0.0350\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.0393 - val_loss: 0.0343\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 4s 133ms/step - loss: 0.0393 - val_loss: 0.0341\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 4s 144ms/step - loss: 0.0392 - val_loss: 0.0355\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 0.0392 - val_loss: 0.0342\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 4s 129ms/step - loss: 0.0392 - val_loss: 0.0352\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 3s 111ms/step - loss: 0.0394 - val_loss: 0.0347\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 4s 124ms/step - loss: 0.0393 - val_loss: 0.0340\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 3s 111ms/step - loss: 0.0398 - val_loss: 0.0365\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 4s 128ms/step - loss: 0.0394 - val_loss: 0.0345\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 3s 104ms/step - loss: 0.0392 - val_loss: 0.0340\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.0389 - val_loss: 0.0347\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 3s 108ms/step - loss: 0.0398 - val_loss: 0.0340\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 3s 106ms/step - loss: 0.0391 - val_loss: 0.0340\n",
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\GRU_Lemmatized_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\deep_learning_models\\GRU_Lemmatized_Fear.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "nn_report = nn_models_obj.nn_model_report(fear_train_df.copy(),'Fear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stemmed</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pearson</th>\n",
       "      <td>0.227450</td>\n",
       "      <td>0.226925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spearman</th>\n",
       "      <td>0.202881</td>\n",
       "      <td>0.222821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Stemmed  Lemmatized\n",
       "Pearson   0.227450    0.226925\n",
       "Spearman  0.202881    0.222821"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(nn_report['RNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stemmed</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pearson</th>\n",
       "      <td>-0.066921</td>\n",
       "      <td>0.007375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spearman</th>\n",
       "      <td>-0.065432</td>\n",
       "      <td>-0.001174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Stemmed  Lemmatized\n",
       "Pearson  -0.066921    0.007375\n",
       "Spearman -0.065432   -0.001174"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(nn_report['LSTM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stemmed</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pearson</th>\n",
       "      <td>0.101004</td>\n",
       "      <td>-0.058105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spearman</th>\n",
       "      <td>0.102477</td>\n",
       "      <td>-0.059346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Stemmed  Lemmatized\n",
       "Pearson   0.101004   -0.058105\n",
       "Spearman  0.102477   -0.059346"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(nn_report['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 2s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = nn_models_obj.nn_models_test_prediction('models\\deep_learning_models\\LSTM_Lemmatized_Sadness.tf',\n",
    "                                        sadness_test_df.copy(),\n",
    "                                        'Lemmatize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pearson': 0.07895379476103466, 'Spearman': 0.07247222681235509}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_obj.eval(sadness_test_df['Intensity'],y_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pearson': 0.03210738354025374, 'Spearman': 0.030989273118014263}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_obj.eval(sadness_test_df['Intensity'],y_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
